import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# > This class is a  Python module for simulation of incentivization mechanism implemented in Blockchain-based Systems
class PyIncentiveBCSystem:
    def __init__(self, node_list, round_list, leader_reward, transaction_fees, leader_node_round_list=None):
        """
        Initialize the class with the provided parameters
        
        :param node_list: A list of node names
        :param round_list: A list of round numbers
        :param leader_reward: The reward for the leader node for each round
        :param transaction_fees: A list of transaction fees for each round
        :param leader_node_round_list: A list of leader nodes for each round (optional)
        """
        self.node_list = node_list
        self.round_list = round_list
        self.leader_reward = leader_reward
        self.transaction_fees = transaction_fees
        self.total_rounds = len(self.round_list)
        self.total_nodes = len(self.node_list)
        self.leader_node_round_list = leader_node_round_list

        if self.leader_node_round_list:
            if len(self.leader_node_round_list) != self.total_rounds:
                raise ValueError("The length of leader_node_round_list must be equal to the length of round_list")
        
        self.data_nodes = []
        self.current_leader_index = 0

        # Initialize the data_nodes list
        for round in range(self.total_rounds):
            if self.leader_node_round_list:
                leader_node = self.leader_node_round_list[round]
            else:
                self.current_leader_index = self.get_next_leader_index(self.current_leader_index)
                leader_node = self.node_list[self.current_leader_index-1]
            
            self.data_nodes.append([self.round_list[round], self.leader_reward, self.transaction_fees[round], leader_node])

    def get_next_leader_index(self, current_leader_index):
        """
        Get the index of the next leader node
        This consensus algorithm consists of electing nodes in turn in an equitable manner during the mining rounds of our simulation. 
        If the current leader is the last node, then the next leader will be the first node. Otherwise, the next leader is the next node
        
        :param current_leader_index: The index of the current leader node
        :return: The index of the next leader node
        """
        if current_leader_index == len(self.node_list) - 1:
            return 0
        else:
            return current_leader_index + 1

    def start_approach_1(self):
        """
        It contain the different processes necessary to implement the first approach

        :return: A dataframe that contains various data regarding rewards of participating nodes
        """
        df = pd.DataFrame(self.data_nodes, columns=['Round', 'Leader Reward', 'Transaction Fees', 'Leader Node'])
        for ind in df.index:
            node_list_per_round = []
            reward_list_per_round = []
            reward_total_per_round = 0
            for node in self.node_list:
                node_list_per_round.append(f"reward_{node}")
                if df['Leader Node'][ind] == node:
                    node_reward = self.leader_reward + df['Transaction Fees'][ind]
                else:
                    node_reward = 0
                reward_list_per_round.append(node_reward)
                reward_total_per_round += node_reward
            node_list_per_round.append("Reward Total per Round")
            reward_list_per_round.append(reward_total_per_round)
            df.loc[ind, node_list_per_round] = reward_list_per_round

        for node in self.node_list:
            df.at['Total', f"reward_{node}"] = df[f"reward_{node}"].sum()

        df.at['Total', "Leader Reward"] = df["Leader Reward"].sum()
        df.at['Total', "Transaction Fees"] = df["Transaction Fees"].sum()
        df.at['Total', "Reward Total per Round"] = df["Reward Total per Round"].sum()
        df.round(2)
        return df

    def start_approach_2(self, saved_part, distributed_part, score_nodes=[]):
        """
        The function contain the different processes necessary to implement the second approach
        
        :param saved_part: The percentage of the generated reward that will be saved for the next round
        :param distributed_part: The percentage of the generated reward that will be distributed to the participating nodes
        :param score_nodes: a list of scores for each node. The scores are between 0 and 1
        :return: A dataframe that contains various data regarding rewards of participating nodes, and a list of scores passed as a parameter or generated by default
        """
        if len(score_nodes) == 0:
            score_nodes = np.random.randint(0, 100, self.total_nodes).astype("float") / 100
        elif len(score_nodes) != self.total_nodes:
            raise TypeError("Sorry, The size of the score list should be the same as the total number of participating nodes (total_nodes)")

        data_nodes = []
        for i in range(self.total_rounds):
            generated_reward = self.transaction_fees[i]
            saved_reward = generated_reward * saved_part
            participating_reward = generated_reward * distributed_part
            participating_reward_per_node = participating_reward / (self.total_nodes - 1)
            penalities_per_round = 0.0
            leader_node = self.data_nodes[i][3]  # Get the leader node from data_nodes
            data_nodes.append([self.round_list[i], self.leader_reward, self.transaction_fees[i], generated_reward, saved_reward, participating_reward, participating_reward_per_node, penalities_per_round, leader_node])

        df2 = pd.DataFrame(data_nodes, columns=['Round', 'Leader Reward', 'Transaction Fees', 'Generated Reward', 'Saved Reward', 'Participating Reward per Round', 'Participating reward per Node', 'Penalities', 'Leader Node'])
        df2.round(2)
        preview_saved_reward = 0
        preview_penalities = 0
        for ind in df2.index:
            node_libelle_list_per_round = []
            node_value_list_per_round = []
            generated_reward = self.transaction_fees[ind] + preview_saved_reward + preview_penalities
            saved_reward = generated_reward * saved_part
            participating_reward = generated_reward * distributed_part
            participating_reward_per_node = participating_reward / (self.total_nodes - 1)
            penalities = 0
            reward_total_per_round = 0
            for node_index, node in enumerate(self.node_list):
                node_libelle_list_per_round.append(f"score_{node}")
                node_libelle_list_per_round.append(f"reward_{node}")
                node_reward = 0
                if df2['Leader Node'][ind] == node:
                    node_reward = self.leader_reward
                else:
                    node_reward = participating_reward_per_node
                score_node = score_nodes[node_index]
                penalitie_per_node = node_reward * (1 - score_node)
                final_reward = node_reward * score_node
                reward_total_per_round += final_reward
                penalities += penalitie_per_node
                node_value_list_per_round.append(score_node)
                node_value_list_per_round.append(final_reward)
            node_libelle_list_per_round.append("Reward Total per Round")
            node_value_list_per_round.append(reward_total_per_round)
            # Update all data for current round
            df2.at[ind, 'Generated Reward'] = generated_reward
            df2.at[ind, 'Saved Reward'] = saved_reward
            df2.at[ind, 'Participating Reward per Round'] = participating_reward
            df2.at[ind, 'Participating reward per Node'] = participating_reward_per_node
            df2.at[ind, 'Penalities'] = penalities
            # Save all data that will be used in next round
            preview_saved_reward = saved_reward
            preview_penalities = penalities
            df2.loc[ind, node_libelle_list_per_round] = node_value_list_per_round

        for node in self.node_list:
            df2.at['Total', f"reward_{node}"] = df2[f"reward_{node}"].sum()

        df2.at['Total', "Leader Reward"] = df2["Leader Reward"].sum()
        df2.at['Total', "Transaction Fees"] = df2["Transaction Fees"].sum()
        df2.at['Total', "Generated Reward"] = df2["Generated Reward"].sum()
        df2.at['Total', "Saved Reward"] = df2["Saved Reward"].sum()
        df2.at['Total', "Penalities"] = df2["Penalities"].sum()
        df2.at['Total', "Participating Reward per Round"] = df2["Participating Reward per Round"].sum()
        df2.at['Total', "Reward Total per Round"] = df2["Reward Total per Round"].sum()  
        df2.round(2)
        return df2, score_nodes

    def compare_approaches(self, df1, df2, score_nodes):
        """
        It takes two dataframes and a list of scores, and returns two dataframes. This function carries  out the elementary processing operations ensuring the comparison of the results obtained by the implemented approaches, preparing and structuring the data necessary to allow the end user to visualize the results in structured forms (dataframe)

        :param df1: the dataframe for the normal approach
        :param df2: the dataframe for the proposed approach
        :param score_nodes: The score of each node used for the proposed approach
        :return: two dataframes.
        """
        list_node_id = []
        reward_list_approach_1 = []
        reward_list_approach_2 = []
        list_round_id = []
        leader_reward_approach_1 = []
        transaction_fees_approach_1 = []
        leader_reward_approach_2 = []
        transaction_fees_approach_2 = []
        previous_saved_part = []
        previous_collected_penalty = []
        
        for node_index, node in enumerate(self.node_list):
            node_id =  self.node_list[node_index]
            reward_list_approach_1.append(df1[f"reward_{node_id}"].iloc[-1])
            reward_list_approach_2.append(df2[f"reward_{node_id}"].iloc[-1])
            list_node_id.append(node)
        all_df_result_1 = pd.DataFrame({'Node': list_node_id, 'Score': score_nodes, 'Normal Approach':reward_list_approach_1, 'Proposed Approach':reward_list_approach_2})

        for round in range(1 , self.total_rounds+1) :
            leader_reward_approach_1.append(df1["Leader Reward"][round-1])
            transaction_fees_approach_1.append(df1["Transaction Fees"][round-1])
            leader_reward_approach_2.append(df2["Leader Reward"][round-1])
            transaction_fees_approach_2.append(df2["Transaction Fees"][round-1])
            if (round > 1):
                previous_saved_part.append(df2["Saved Reward"][round-2])
                previous_collected_penalty.append(df2["Penalities"][round-2])
            else:
                previous_saved_part.append(0.0)
                previous_collected_penalty.append(0.0)
            list_round_id.append(round)

        all_df_result_2 = pd.DataFrame({'Round': list_round_id, 
        'Leader Reward 1': leader_reward_approach_1, 
        'Transaction Fees 1':transaction_fees_approach_1, 
        'Leader Reward 2':leader_reward_approach_2,
        'Transaction Fees 2':transaction_fees_approach_2,
        'Saved Reward':previous_saved_part,
        'Penalities':previous_collected_penalty})
        return all_df_result_1, all_df_result_2

    def plot_graph_1(self, all_df_result, with_table = False):
        """
        It creates a graph with two bars and a scatter plot to represent all Rewards received using Normal and Proposed Approach
        
        illustrate them in ergonomic graphs 

        :param all_df_result: This is the dataframe that contains the data to be plotted
        :param with_table: If you want to show the table below the graph, set this to True, defaults to
        False (optional)
        :return: A figure object
        """
        row_count = 1
        specs_config=[[{"type": "xy"}]]
        height_config = 400
        if (with_table):
            row_count = 2
            specs_config=[[{"type": "xy"}], [{"type": "domain"}]]
            height_config = 800
        
        fig = make_subplots(rows=row_count, cols=1, specs=specs_config, vertical_spacing=0.1)

        nodes = all_df_result["Node"].values.tolist()
        scores = [x*10 for x in all_df_result["Score"].values.tolist()]
        fig.add_trace(go.Bar(name='Normal Approach', xaxis='x2', yaxis='y2', x=nodes, y=all_df_result["Normal Approach"].values.tolist(),texttemplate= "%{value:.4s}", textposition='inside',marker_color='rgb(55, 83, 109)'),row=1, col=1)
        fig.add_trace(go.Bar(name='Proposed Approach', xaxis='x2', yaxis='y2', x=nodes, y=all_df_result["Proposed Approach"].values.tolist(),text=all_df_result["Score"].values.tolist(),textposition="inside", texttemplate= "%{value:.4s} <br> Score: %{text}", marker_color='rgb(26, 118, 255)'),row=1, col=1)
        fig.add_trace(go.Scatter(name="Score",xaxis='x2', yaxis='y2', x=nodes, y = scores, text = all_df_result["Score"].values.tolist(), mode="markers",  marker=dict(color="crimson", size=12),  hovertemplate="%{text}"),row=1, col=1)
        #Table section
        if (with_table):
            fmt = np.array(['N/A','.3f','.3f','.3f'])
            headercolor = 'midnightblue'
            fig.add_trace(go.Table(
                header=dict(
                    values=[['<b>{}</b>'.format(h)] for h in all_df_result.columns],
                    fill_color= headercolor,
                    line_color='darkslategray',
                    font=dict(color='white', size= 12)
                ),
                cells=dict(
                    values=all_df_result.values.T,
                    format=fmt,
                    height=25
                )), row=2, col=1) 

        fig.update_layout(title='Rewards received using Normal and Proposed Approach',
                        hovermode='closest',
                        xaxis=dict(title='Nodes', autorange=True, tickmode = 'linear'),
                        yaxis=dict(title='Rewards', autorange=True),
                        barmode='group',
                        height=height_config,
                        margin=dict(l=20, r=30, t=40, b=00))
        return fig

    def plot_graph_2(self, all_df_result, with_table = False):
        """
        The graph generated by this function is a bar chart with two bars per round, one for each approach, and a table below it.
        This graph shows a comparison of raw rewards (without deduction of penalties) generated by the normal approach and the proposed approach.
        
        :param all_df_result: the dataframe containing the data to be plotted
        :param with_table: if True, the table will be displayed below the graph, defaults to False
        (optional)
        :return: A plotly figure object
        """
        row_count = 1
        specs_config=[[{"type": "xy"}]]
        height_config = 600
        if (with_table):
            row_count = 2
            specs_config=[[{"type": "xy"}], [{"type": "domain"}]]
            height_config = height_config*1.7

        fig = make_subplots(rows=row_count, cols=1, specs=specs_config, vertical_spacing=0.09)
        # Add table data
        index = pd.RangeIndex(1, self.total_rounds+1)
        new_df = all_df_result.set_index('Round')

        df = pd.concat([pd.DataFrame(new_df,index=index,columns=["Leader Reward 1", "Transaction Fees 1"]),
                pd.DataFrame(new_df,index=index,columns=["Leader Reward 2", "Transaction Fees 2", "Saved Reward", "Penalities"]
                ),], axis=1, keys=["Normal Approach", "Proposed Approach"]   )
        colors = {"Normal Approach": {
                "Leader Reward 1": "#F28F1D",
                "Transaction Fees 1": "#F6C619",
                "Revenue3": "#FADD75",
                },"Proposed Approach": {
                "Leader Reward 2": "#2B6045",
                "Transaction Fees 2": "#5EB88A",
                "Saved Reward": "#9ED4B9",
                "Penalities": "#9EDFFF", } }
        for i, t in enumerate(colors):
            for j, col in enumerate(df[t].columns):
                if (df[t][col] == 0).all():
                    continue
                fig.add_bar(
                    x=df.index,
                    y=df[t][col],
                    yaxis=f"y{i + 1}",
                    offsetgroup=str(i),
                    offset=(i - 1) * 0.4,
                    width=0.4,
                    legendgroup=t,
                    legendgrouptitle_text=t,
                    name=col,
                    marker_color=colors[t][col],
                    marker_line=dict(width=2, color="#333"),
                    hovertemplate="%{y}<extra></extra>",
                    texttemplate= "%{value:.4s}", textposition='inside' )

        if (with_table):
            fmt = np.array(['.0f','.3f','.3f','.3f','.3f','.3f'])
            headercolor = 'midnightblue'
            fig.add_trace(go.Table(
                header=dict(
                    values=[['<b>{}</b>'.format(h)] for h in all_df_result.columns],
                    fill_color=headercolor,
                    line_color='darkslategray',
                    font=dict(color='white', size=12),
                ),
                cells=dict(
                    values=all_df_result.values.T,
                    format=fmt,
                    height=20
                )), row=2, col=1) 

        fig.update_layout(
            title_text="Raw rewards repartition using normal and proposed approach",
            barmode="relative",
            yaxis_showticklabels=False,
            yaxis_showgrid=False,
            yaxis_range=[0, df.groupby(axis=1, level=0).sum().max().max() * 1.5],
            xaxis=dict(title='Rounds', tickmode = 'linear'),
            yaxis=dict(title='Rewards'),
            yaxis2=go.layout.YAxis(
                visible=True,
                matches="y",
                overlaying="y",
                anchor="x",
            ),
            font=dict(size=14),
            legend_x=0,
            legend_y=1,
            legend_orientation="h",
            hovermode="x",
            margin=dict(b=0,t=35,l=10,r=10),
            height=height_config
        )
        return fig

    def plot_graph_3(self, all_df_result):
        """
        By generating two pie charts side by side using plotly, This graph makes it possible to represent the distribution of raw rewards according to their origin during all round of our simulation.
               
        :param all_df_result: This is the dataframe that contains the data for the graph
        :return: A plotly figure object
        """
        all_df_result_2_total =  pd.DataFrame()
        all_df_result_2_total.at['Total', "Leader Reward 1"] = all_df_result["Leader Reward 1"].sum()
        all_df_result_2_total.at['Total', "Transaction Fees 1"] = all_df_result["Transaction Fees 1"].sum()
        all_df_result_2_total.at['Total', "Leader Reward 2"] = all_df_result["Leader Reward 2"].sum()
        all_df_result_2_total.at['Total', "Transaction Fees 2"] = all_df_result["Transaction Fees 2"].sum()
        all_df_result_2_total.at['Total', "Penalities"] = all_df_result["Penalities"].sum()
        all_df_result_2_total.at['Total', "Saved Reward"] = all_df_result["Saved Reward"].sum()

        list_appraoch_1 = all_df_result_2_total.loc[all_df_result_2_total.index[-1], ["Leader Reward 1","Transaction Fees 1"]].values.tolist()+[0,0]
        list_approach_2 = all_df_result_2_total.loc[all_df_result_2_total.index[-1], ["Leader Reward 2","Transaction Fees 2","Penalities","Saved Reward"]].values.tolist()
        labels = ["Leader Reward","Transaction Fees","Penalities","Saved Reward"]

        fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])
        fig.add_trace(go.Pie(labels=labels, values=list_appraoch_1 , name="Normal Approach", title="Normal Approach", titleposition="bottom center", texttemplate="%{percent:.0%}", rotation=90), 1, 1)
        fig.add_trace(go.Pie(labels=labels, values=list_approach_2, name="Proposed Approach", title="Proposed Approach", titleposition="bottom center", texttemplate="%{percent:.0%}"), 1, 2)

        fig.update_traces(hole=.4, hoverinfo="label+value+name")
        fig.update_layout(
            title_text="Rewards received using Normal and Proposed Approach by %",
            margin=dict(b=0,t=35,l=10,r=10),
            height=400
        )
        return fig